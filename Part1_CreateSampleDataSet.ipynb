{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46efc6e",
   "metadata": {},
   "source": [
    "# Data Set\n",
    "\n",
    "**Source:** FBI National Stolen Art File\\\n",
    "**Retrieval Date:** August 25, 2024\\\n",
    "**About:** A listing of the paintings, statuary, and other forms of fine art in the FBI's database of stolen artwork and culturally-significant property.].\\\n",
    "**Source URL:** [Access web database here.](https://artcrimes.fbi.gov/nsaf-view?searchText=&crimeCategory=)\\\n",
    "**Github Source:** The data taken from the FBI National Stolen Art File will be saved in several formats in this repo, and different formatted versions will be used for this project dependent on the transformation and type of test being applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dbd1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a17c8909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/Alli_1/opt/anaconda3/lib/python3.9/site-packages (1.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/Alli_1/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/Alli_1/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/Alli_1/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Users/Alli_1/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.24.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2273184",
   "metadata": {},
   "source": [
    "THIS IS A WORKING FILE TO PREP.... CODE IS NOT CURRENTLY BEING RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18fc335",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/fbi_stolen_art_research/FBI_Messy_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e720fd97",
   "metadata": {},
   "source": [
    "Briefly making sure the data loaded correctly and looks as expected..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d211704",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49671221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7552c13f",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "\n",
    "My dataset is too big and too messy to properly clean in its current size due to the amount of \"hand\" editing that needs to be done and additional art historical context columns that will need to be added. So, I need to create a sample. So it isn't too imbalanced, I'm going to do a stratified sample based on the \"Category\" feature. This is one of the data-points most consistently incldued in the original web database data, and one that I will apply the least transformations to down the road. As such, it will be the category used as the basis for the stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## briefly getting a closer handle on the 'category' feature\n",
    "df1['category'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking here to get a handle of the general districution among the category types as a kick-off\n",
    "df1['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d93931",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a function to do a stratified sample\n",
    "\n",
    "def stratified_sampling(df, strata_col, sample_size):\n",
    "    groups = df.groupby(strata_col)\n",
    "    sample = pd.DataFrame()\n",
    "    \n",
    "    for _, group in groups:\n",
    "        stratum_sample = group.sample(frac=sample_size, replace=False, random_state=7)\n",
    "        sample = sample.append(stratum_sample)\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67768d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = stratified_sampling(df1, 'category', 0.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb1bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## making sure it worked correctly\n",
    "\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd50d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming that the stratification worked\n",
    "\n",
    "def get_value_counts(array):\n",
    "  unique, counts = np.unique(array, return_counts=True)\n",
    "  total_count = counts.sum()\n",
    "  percent = counts / total_count\n",
    "\n",
    "  print(\"Unique Values:\", unique)\n",
    "  print(\"Counts:\", counts)\n",
    "  print(\"Proportion:\", percent)\n",
    "\n",
    "print(\"\", 'Sample Distribution')\n",
    "get_value_counts(sample_df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e829218f",
   "metadata": {},
   "source": [
    "### Step 3: Export\n",
    "\n",
    "Since everything looks as it should, I am next going to export this sample as a new CSV which I will be able to further clean both by hand, and later merge with necessary data in a separate notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29d1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv('PI_Messy_Sample_Data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
